name: Upload CSV to Google Cloud

on:
  pull_request:
    branches:
      - main
    types: [closed]


jobs:
  upload_csv_to_google_cloud:
    runs-on: cdi-runner
    env:
      FOLDER_NAME: data-${{ github.run_id }}-${{ github.run_attempt }}

    steps:
    - uses: actions/checkout@v4

    - name: setup-gcloud
      uses: 'google-github-actions/auth@v2'
      with:
        credentials_json: '${{ secrets.GOOGLE_CREDENTIALS }}'

    # We could also consider changing the output of the data processing process,
    # in a way that the needed files for back-end are group into their own folder
    # so that we can upload the whole folder to the cloud storage
    - id: 'upload-file-agency'
      uses: 'google-github-actions/upload-cloud-storage@v2'
      with:
        path: '${{ vars.DATA_DIR }}/agency_reference_list.csv'
        destination: '${{ vars.DATA_BUCKET }}/${{ env.FOLDER_NAME }}'
        process_gcloudignore: false

    - id: 'upload-file-personnel'
      uses: 'google-github-actions/upload-cloud-storage@v2'
      with:
        path: '${{ vars.DATA_DIR }}/personnel.csv'
        destination: '${{ vars.DATA_BUCKET }}/${{ env.FOLDER_NAME }}'
        process_gcloudignore: false

    - id: 'upload-file-allegation'
      uses: 'google-github-actions/upload-cloud-storage@v2'
      with:
        path: '${{ vars.DATA_DIR }}/allegation.csv'
        destination: '${{ vars.DATA_BUCKET }}/${{ env.FOLDER_NAME }}'
        process_gcloudignore: false

    - id: 'upload-file-data_brady'
      uses: 'google-github-actions/upload-cloud-storage@v2'
      with:
        path: '${{ vars.DATA_DIR }}/brady.csv'
        destination: '${{ vars.DATA_BUCKET }}/${{ env.FOLDER_NAME }}'
        process_gcloudignore: false

    - id: 'upload-file-data_use-of-force'
      uses: 'google-github-actions/upload-cloud-storage@v2'
      with:
        path: '${{ vars.DATA_DIR }}/use_of_force.csv'
        destination: '${{ vars.DATA_BUCKET }}/${{ env.FOLDER_NAME }}'
        process_gcloudignore: false

    - id: 'upload-file-data_citizens'
      uses: 'google-github-actions/upload-cloud-storage@v2'
      with:
        path: '${{ vars.DATA_DIR }}/citizens.csv'
        destination: '${{ vars.DATA_BUCKET }}/${{ env.FOLDER_NAME }}'
        process_gcloudignore: false

    - id: 'upload-file-data_appeals'
      uses: 'google-github-actions/upload-cloud-storage@v2'
      with:
        path: '${{ vars.DATA_DIR }}/appeals.csv'
        destination: '${{ vars.DATA_BUCKET }}/${{ env.FOLDER_NAME }}'
        process_gcloudignore: false

    - id: 'upload-file-data_event'
      uses: 'google-github-actions/upload-cloud-storage@v2'
      with:
        path: '${{ vars.DATA_DIR }}/event.csv'
        destination: '${{ vars.DATA_BUCKET }}/${{ env.FOLDER_NAME }}'
        process_gcloudignore: false

    - id: 'upload-file-data_documents'
      uses: 'google-github-actions/upload-cloud-storage@v2'
      with:
        path: '${{ vars.DATA_DIR }}/documents.csv'
        destination: '${{ vars.DATA_BUCKET }}/${{ env.FOLDER_NAME }}'
        process_gcloudignore: false

    - id: 'upload-file-data_post-officer-history'
      uses: 'google-github-actions/upload-cloud-storage@v2'
      with:
        path: '${{ vars.DATA_DIR }}/post_officer_history.csv'
        destination: '${{ vars.DATA_BUCKET }}/${{ env.FOLDER_NAME }}'
        process_gcloudignore: false

    - id: 'upload-file-data_person'
      uses: 'google-github-actions/upload-cloud-storage@v2'
      with:
        path: '${{ vars.DATA_DIR }}/person.csv'
        destination: '${{ vars.DATA_BUCKET }}/${{ env.FOLDER_NAME }}'
        process_gcloudignore: false

    # Upload agency-specific input CSVs and save URLs
    - id: 'upload-agency-csvs'
      uses: 'google-github-actions/upload-cloud-storage@v2'
      with:
        path: '${{ vars.DATA_DIR }}/fuse_agency'
        destination: '${{ vars.DATA_BUCKET }}/${{ env.FOLDER_NAME }}/fuse_agency'
        process_gcloudignore: false
        recursive: true

    - name: 'Generate CSV URLs file'
      run: |
        echo "Generating URLs for uploaded files..."
        echo "{" > csv_urls.json
        
        # List all uploaded CSVs and generate their public URLs
        # Focus on agency-specific CSVs from fuse_agency directory
        gsutil ls "gs://${{ vars.DATA_BUCKET }}/${{ env.FOLDER_NAME }}/fuse_agency/per_*.csv" | while read -r file; do
          filename=$(basename "$file")
          echo "  \"${filename}\": \"https://storage.googleapis.com/${file#gs://}\"," >> csv_urls.json
        done
        
        # Remove trailing comma and close JSON
        sed -i '$ s/,$//' csv_urls.json
        echo "}" >> csv_urls.json
        
        # Upload the URLs file back to cloud storage
        gsutil cp csv_urls.json "gs://${{ vars.DATA_BUCKET }}/${{ env.FOLDER_NAME }}/csv_urls.json"
        
        # Make the CSV files and URLs file publicly readable
        gsutil -m acl ch -r -u AllUsers:R "gs://${{ vars.DATA_BUCKET }}/${{ env.FOLDER_NAME }}/fuse_agency/per_*.csv"
        gsutil acl ch -u AllUsers:R "gs://${{ vars.DATA_BUCKET }}/${{ env.FOLDER_NAME }}/csv_urls.json"
    - name: 'Trigger Backend staging data import'
      id: 'trigger-backend-staging'
      run: |
        curl -X POST -H "Content-Type: application/json" \
                    -H "X-Api-Key: ${{ secrets.IPNO_API_KEY }}" \
                    -d '{"folder_name": "${{ env.FOLDER_NAME }}"}' \
                    https://api-staging.llead.co/api/data/import/

    - name: 'Trigger Backend production data import'
      id: 'trigger-backend-production'
      run: |
        curl -X POST -H "Content-Type: application/json" \
                    -H "X-Api-Key: ${{ secrets.IPNO_API_KEY }}" \
                    -d '{"folder_name": "${{ env.FOLDER_NAME }}"}' \
                    https://api.llead.co/api/data/import/
