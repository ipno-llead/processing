name: Upload CSV to Google Cloud

on:
  pull_request:
    branches:
      - main
    types: [closed]


jobs:
  upload_csv_to_google_cloud:
    runs-on: cdi-runner
    env:
      FOLDER_NAME: data-${{ github.run_id }}-${{ github.run_attempt }}

    steps:
    - uses: actions/checkout@v4

    - name: Set up Google Cloud SDK
      uses: 'google-github-actions/setup-gcloud@v2'
      with:
        version: 'latest'
        install_components: 'gsutil'

    - name: Verify Google Cloud setup
      run: |
        echo "Verifying Google Cloud SDK installation..."
        gcloud --version
        echo "Verifying gsutil installation..."
        which gsutil || (sudo apt-get update && sudo apt-get install -y google-cloud-sdk-gsutil)
        gsutil --version

    - name: Authenticate with Google Cloud
      uses: 'google-github-actions/auth@v2'
      with:
        credentials_json: '${{ secrets.GOOGLE_CREDENTIALS_UPDATE }}'
        export_environment_variables: true
        create_credentials_file: true
        
    # We could also consider changing the output of the data processing process,
    # in a way that the needed files for back-end are group into their own folder
    # so that we can upload the whole folder to the cloud storage
    - id: 'upload-file-agency'
      uses: 'google-github-actions/upload-cloud-storage@v2'
      with:
        path: '${{ vars.DATA_DIR }}/agency_reference_list.csv'
        destination: '${{ vars.DATA_BUCKET }}/${{ env.FOLDER_NAME }}'
        process_gcloudignore: false

    - id: 'upload-file-personnel'
      uses: 'google-github-actions/upload-cloud-storage@v2'
      with:
        path: '${{ vars.DATA_DIR }}/personnel.csv'
        destination: '${{ vars.DATA_BUCKET }}/${{ env.FOLDER_NAME }}'
        process_gcloudignore: false

    - id: 'upload-file-allegation'
      uses: 'google-github-actions/upload-cloud-storage@v2'
      with:
        path: '${{ vars.DATA_DIR }}/allegation.csv'
        destination: '${{ vars.DATA_BUCKET }}/${{ env.FOLDER_NAME }}'
        process_gcloudignore: false

    - id: 'upload-file-data_brady'
      uses: 'google-github-actions/upload-cloud-storage@v2'
      with:
        path: '${{ vars.DATA_DIR }}/brady.csv'
        destination: '${{ vars.DATA_BUCKET }}/${{ env.FOLDER_NAME }}'
        process_gcloudignore: false

    - id: 'upload-file-data_use-of-force'
      uses: 'google-github-actions/upload-cloud-storage@v2'
      with:
        path: '${{ vars.DATA_DIR }}/use_of_force.csv'
        destination: '${{ vars.DATA_BUCKET }}/${{ env.FOLDER_NAME }}'
        process_gcloudignore: false

    - id: 'upload-file-data_citizens'
      uses: 'google-github-actions/upload-cloud-storage@v2'
      with:
        path: '${{ vars.DATA_DIR }}/citizens.csv'
        destination: '${{ vars.DATA_BUCKET }}/${{ env.FOLDER_NAME }}'
        process_gcloudignore: false

    - id: 'upload-file-data_appeals'
      uses: 'google-github-actions/upload-cloud-storage@v2'
      with:
        path: '${{ vars.DATA_DIR }}/appeals.csv'
        destination: '${{ vars.DATA_BUCKET }}/${{ env.FOLDER_NAME }}'
        process_gcloudignore: false

    - id: 'upload-file-data_event'
      uses: 'google-github-actions/upload-cloud-storage@v2'
      with:
        path: '${{ vars.DATA_DIR }}/event.csv'
        destination: '${{ vars.DATA_BUCKET }}/${{ env.FOLDER_NAME }}'
        process_gcloudignore: false

    - id: 'upload-file-data_documents'
      uses: 'google-github-actions/upload-cloud-storage@v2'
      with:
        path: '${{ vars.DATA_DIR }}/documents.csv'
        destination: '${{ vars.DATA_BUCKET }}/${{ env.FOLDER_NAME }}'
        process_gcloudignore: false

    - id: 'upload-file-data_post-officer-history'
      uses: 'google-github-actions/upload-cloud-storage@v2'
      with:
        path: '${{ vars.DATA_DIR }}/post_officer_history.csv'
        destination: '${{ vars.DATA_BUCKET }}/${{ env.FOLDER_NAME }}'
        process_gcloudignore: false

    - id: 'upload-file-data_person'
      uses: 'google-github-actions/upload-cloud-storage@v2'
      with:
        path: '${{ vars.DATA_DIR }}/person.csv'
        destination: '${{ vars.DATA_BUCKET }}/${{ env.FOLDER_NAME }}'
        process_gcloudignore: false

    # Upload agency-specific input CSVs and save URLs
    - id: 'upload-agency-csvs'
      uses: 'google-github-actions/upload-cloud-storage@v2'
      with:
        path: '/runner/_work/data/data/fuse_agency'
        destination: '${{ vars.DATA_BUCKET }}/${{ env.FOLDER_NAME }}/fuse_agency'
        process_gcloudignore: false
        recursive: true

    # Verify files exist before processing
    - name: 'Verify CSV files'
      run: |
        echo "Verifying CSV files exist..."
        if [ ! -d "/runner/_work/data/data/fuse_agency" ]; then
          echo "Error: fuse_agency directory not found"
          exit 1
        fi
        
        # List all agency CSVs to verify they exist
        echo "Found agency CSV files:"
        find /runner/_work/data/data/fuse_agency -name "*.csv" -type f -ls

    - name: 'Generate CSV URLs file'
      run: |
        echo "Generating URLs for uploaded files..."
        echo "{" > csv_urls.json
        
        # Verify files were uploaded successfully
        echo "Verifying uploaded files in Google Cloud Storage..."
        UPLOADED_FILES=$(gsutil ls "gs://${{ vars.DATA_BUCKET }}/${{ env.FOLDER_NAME }}/fuse_agency/fuse_agency/*.csv")
        if [ -z "$UPLOADED_FILES" ]; then
          echo "Error: No agency CSV files found in cloud storage"
          exit 1
        fi
        
        echo "Found the following uploaded files:"
        echo "$UPLOADED_FILES"
        
        # Generate URLs for verified files
        echo "Generating public URLs..."
        echo "$UPLOADED_FILES" | while read -r file; do
          if [ ! -z "$file" ]; then
            filename=$(basename "$file")
            filetype=${filename%%_*}  # Extract type (per, com, uof, etc.)
            echo "Processing $filename (type: $filetype)..."
            echo "  \"${filename}\": \"https://storage.googleapis.com/${file#gs://}\"," >> csv_urls.json
          fi
        done
        
        # Remove trailing comma and close JSON
        sed -i '$ s/,$//' csv_urls.json
        echo "}" >> csv_urls.json
        
        # Verify JSON format
        echo "Verifying JSON format..."
        if ! jq empty csv_urls.json; then
          echo "Error: Invalid JSON format"
          exit 1
        fi
        
        echo "Uploading URLs file and setting permissions..."
        # Upload the URLs file back to cloud storage
        gsutil cp csv_urls.json "gs://${{ vars.DATA_BUCKET }}/${{ env.FOLDER_NAME }}/csv_urls.json"
        
        # Make all CSV files and URLs file publicly readable
        gsutil iam ch allUsers:objectViewer gs://${{ vars.DATA_BUCKET }}/${{ env.FOLDER_NAME }}/fuse_agency/
        gsutil iam ch allUsers:objectViewer gs://${{ vars.DATA_BUCKET }}/${{ env.FOLDER_NAME }}/csv_urls.json
                
        echo "URL generation and permission setting completed successfully"
    - name: 'Trigger Backend staging data import'
      id: 'trigger-backend-staging'
      run: |
        curl -X POST -H "Content-Type: application/json" \
                    -H "X-Api-Key: ${{ secrets.IPNO_API_KEY }}" \
                    -d '{"folder_name": "${{ env.FOLDER_NAME }}"}' \
                    https://api-staging.llead.co/api/data/import/

    - name: 'Trigger Backend production data import'
      id: 'trigger-backend-production'
      run: |
        curl -X POST -H "Content-Type: application/json" \
                    -H "X-Api-Key: ${{ secrets.IPNO_API_KEY }}" \
                    -d '{"folder_name": "${{ env.FOLDER_NAME }}"}' \
                    https://api.llead.co/api/data/import/