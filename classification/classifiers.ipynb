{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import gensim\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import nltk; nltk.download('stopwords')\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import re\n",
    "import warnings\n",
    "from pprint import pprint\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import seaborn as sns\n",
    "%config InlineBackend.figure_formats = ['retina']\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import fbeta_score\n",
    "import matplotlib.pyplot as plt\n",
    "from top2vec import Top2Vec\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import TfidfModel\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipynb in c:\\users\\pc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (0.5.1)\n"
     ]
    }
   ],
   "source": [
    "import ipynb\n",
    "from ipynb.fs.full.topic_models import split_data, lda_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, test_data = split_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1499\n",
       "1     501\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################### lda ##############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_train_vecs = lda_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel.load(\"models/lda_train.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_docs = test_data.allegation_desc\n",
    "\n",
    "def lemmatization(descs, allowed_pos_tags=[\"NOUN\", \"ADJ\", \"VERB\", \"ADV\"]):\n",
    "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "    final_text = []\n",
    "    for desc in descs:\n",
    "        doc = nlp(desc)\n",
    "        new_text = \" \".join([token.lemma_ for token in doc if token.pos_ in allowed_pos_tags])\n",
    "        final_text.append(new_text)\n",
    "    return (final_text)\n",
    "\n",
    "lemmatized_texts = lemmatization(test_docs)\n",
    "\n",
    "def gen_words(texts):\n",
    "    final = []\n",
    "    for text in texts:\n",
    "        new = gensim.utils.simple_preprocess(text, deacc=True)\n",
    "        final.append(new)\n",
    "    return (final)\n",
    "\n",
    "data_words = gen_words(lemmatized_texts)\n",
    "\n",
    "bigram_phrases = gensim.models.Phrases(data_words, min_count=5, threshold=50)\n",
    "trigram_phrases = gensim.models.Phrases(bigram_phrases[data_words], threshold=50)\n",
    "\n",
    "bigram = gensim.models.phrases.Phraser(bigram_phrases)\n",
    "trigram = gensim.models.phrases.Phraser(trigram_phrases)\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return list(bigram[doc] for doc in texts)\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return list(trigram[bigram[doc]] for doc in texts)\n",
    "\n",
    "data_bigrams = make_bigrams(data_words)\n",
    "data_bigrams_trigrams = make_trigrams(data_words)\n",
    "\n",
    "id2word = corpora.Dictionary(data_bigrams_trigrams)\n",
    "\n",
    "texts = data_bigrams_trigrams\n",
    "\n",
    "test_corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "tdidf = TfidfModel(test_corpus, id2word=id2word)\n",
    "\n",
    "low_value = 0.03\n",
    "words = []\n",
    "words_missing_in_tdif = []\n",
    "\n",
    "for i in range(0, len(test_corpus)):\n",
    "    bow = test_corpus[i]\n",
    "    low_value_words = []\n",
    "    tdif_ids = [id for id, value in tdidf[bow]]\n",
    "    bow_ids = [id for id, value in bow]\n",
    "    low_value_words = [id for id, value in tdidf[bow] if value < low_value]\n",
    "    drops = low_value_words+words_missing_in_tdif\n",
    "    for item in drops:\n",
    "        words.append(id2word[item])\n",
    "    words_missing_in_tdif = [id for id in bow_ids if id not in tdif_ids]\n",
    "\n",
    "    new_bow = [b for b in bow if b[0] not in low_value_words and b[0] not in words_missing_in_tdif]\n",
    "    test_corpus[i] = new_bow\n",
    "\n",
    "def get_test_vecs():\n",
    "    lda_test_vecs = []\n",
    "    for i in range(len(test_docs)):\n",
    "        top_topics = lda_model.get_document_topics(test_corpus[i], minimum_probability=0.0)\n",
    "        topic_vec = [top_topics[i][1] for i in range(10)]\n",
    "        topic_vec.extend([len(test_docs.iloc[i])]) # length review\n",
    "        lda_test_vecs.append(topic_vec)\n",
    "    return lda_test_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_test_vecs = get_test_vecs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################### top2vec ##############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_2_vec_model_train = Top2Vec.load(\"models/noso\")\n",
    "top_train_vecs = top_2_vec_model_train.document_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_test_data_to_list(data):\n",
    "    test_docs = [x for x in data[\"allegation_desc\"]]\n",
    "    return test_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-30 13:15:52,494 - top2vec - INFO - Pre-processing documents for training\n",
      "2022-11-30 13:15:52,655 - top2vec - INFO - Creating joint document/word embedding\n",
      "2022-11-30 13:15:57,575 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "2022-11-30 13:16:10,128 - top2vec - INFO - Finding dense areas of documents\n",
      "2022-11-30 13:16:10,285 - top2vec - INFO - Finding topics\n"
     ]
    }
   ],
   "source": [
    "test_docs = convert_test_data_to_list(test_data)\n",
    "\n",
    "top_2_vec_model_test = Top2Vec(test_docs, embedding_model_path=\"models/noso\")\n",
    "\n",
    "top_test_vecs = top_2_vec_model_test.document_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################# regress #################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.array(top_test_vecs)\n",
    "y_test = np.array(test_data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(top_train_vecs)\n",
    "y = np.array(training_data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\_linesearch.py:305: LineSearchWarning:\n",
      "\n",
      "The line search algorithm did not converge\n",
      "\n",
      "c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\_linesearch.py:305: LineSearchWarning:\n",
      "\n",
      "The line search algorithm did not converge\n",
      "\n",
      "c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\_linesearch.py:305: LineSearchWarning:\n",
      "\n",
      "The line search algorithm did not converge\n",
      "\n",
      "c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\_linesearch.py:305: LineSearchWarning:\n",
      "\n",
      "The line search algorithm did not converge\n",
      "\n",
      "c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\_linesearch.py:305: LineSearchWarning:\n",
      "\n",
      "The line search algorithm did not converge\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Val f1: 0.535 +- 0.017\n",
      "Logisitic Regression SGD Val f1: 0.533 +- 0.022\n",
      "SVM Huber Val f1: 0.641 +- 0.010\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(5, shuffle=True, random_state=48)\n",
    "cv_lr_f1, cv_lrsgd_f1, cv_svcsgd_f1,  = [], [], []\n",
    "\n",
    "for train_ind, val_ind in kf.split(X, y):\n",
    "    X_train, y_train = X[train_ind], y[train_ind]\n",
    "    X_val, y_val = X[val_ind], y[val_ind]\n",
    "    \n",
    "    # Scale Data\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scale = scaler.fit_transform(X_train)\n",
    "    X_val_scale = scaler.transform(X_val)\n",
    "\n",
    "    # Logisitic Regression\n",
    "    lr = LogisticRegression(\n",
    "        class_weight= 'balanced',\n",
    "        solver='newton-cg',\n",
    "        fit_intercept=True,\n",
    "    ).fit(X_train_scale, y_train)\n",
    "\n",
    "    test_data[\"lr_scores\"] = lr.predict(scaler.transform(X_test))\n",
    "    test_data[[\"lr_scores_prob_1\"]] = lr.predict_proba(scaler.transform(X_test))[:, 1]\n",
    "\n",
    "    y_pred = lr.predict(scaler.transform(X_val_scale))\n",
    "    cv_lr_f1.append(f1_score(y_val, y_pred, average='weighted'))\n",
    "    \n",
    "    # Logistic Regression Mini-Batch SGD\n",
    "    sgd = linear_model.SGDClassifier(\n",
    "        max_iter=1000,\n",
    "        tol=1e-3,\n",
    "        loss='log',\n",
    "        class_weight='balanced'\n",
    "    ).fit(X_train_scale, y_train)\n",
    "\n",
    "    test_data[\"sgd_scores\"] = sgd.predict(scaler.transform(X_test))\n",
    "    test_data[[\"sgd_scores_prob_1\"]] = sgd.predict_proba(scaler.transform(X_test))[:, 1]\n",
    "\n",
    "    y_pred = sgd.predict(X_val_scale)\n",
    "    cv_lrsgd_f1.append(f1_score(y_val, y_pred, average=\"weighted\"))\n",
    "    \n",
    "    # SGD Modified Huber\n",
    "    sgd_huber = linear_model.SGDClassifier(\n",
    "        max_iter=1000,\n",
    "        tol=1e-3,\n",
    "        alpha=20,\n",
    "        loss='modified_huber',\n",
    "        class_weight='balanced'\n",
    "    ).fit(X_train_scale, y_train)\n",
    "\n",
    "    test_data[\"sgd_huber_scores\"] = sgd_huber.predict(scaler.transform(X_test))\n",
    "    test_data[[\"sgd_huber_scores_prob_1\"]] = sgd_huber.predict_proba(scaler.transform(X_test))[:, 1]\n",
    "    \n",
    "    y_pred = sgd_huber.predict(X_val_scale)\n",
    "    cv_svcsgd_f1.append(f1_score(y_val, y_pred, average=\"weighted\"))\n",
    "\n",
    "print(f'Logistic Regression Val f1: {np.mean(cv_lr_f1):.3f} +- {np.std(cv_lr_f1):.3f}')\n",
    "print(f'Logisitic Regression SGD Val f1: {np.mean(cv_lrsgd_f1):.3f} +- {np.std(cv_lrsgd_f1):.3f}')\n",
    "print(f'SVM Huber Val f1: {np.mean(cv_svcsgd_f1):.3f} +- {np.std(cv_svcsgd_f1):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>allegation_desc</th>\n",
       "      <th>meta</th>\n",
       "      <th>label</th>\n",
       "      <th>target</th>\n",
       "      <th>lr_scores</th>\n",
       "      <th>lr_scores_prob_1</th>\n",
       "      <th>sgd_scores</th>\n",
       "      <th>sgd_scores_prob_1</th>\n",
       "      <th>sgd_huber_scores</th>\n",
       "      <th>sgd_huber_scores_prob_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1078</th>\n",
       "      <td>officer worked a paid detail in plain clothes ...</td>\n",
       "      <td>2014-0105-d</td>\n",
       "      <td>nan</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.098106</td>\n",
       "      <td>0</td>\n",
       "      <td>1.023363e-12</td>\n",
       "      <td>0</td>\n",
       "      <td>0.264397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1020</th>\n",
       "      <td>complaint was via webmail. police were called ...</td>\n",
       "      <td>2014-0001-n</td>\n",
       "      <td>nan</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001504</td>\n",
       "      <td>0</td>\n",
       "      <td>4.948917e-17</td>\n",
       "      <td>0</td>\n",
       "      <td>0.354349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1374</th>\n",
       "      <td>officer accidentally left her service weapon u...</td>\n",
       "      <td>2014-0565-r</td>\n",
       "      <td>nan</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000729</td>\n",
       "      <td>0</td>\n",
       "      <td>1.976547e-26</td>\n",
       "      <td>0</td>\n",
       "      <td>0.273947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1177</th>\n",
       "      <td>the complainant stated she was leaving her res...</td>\n",
       "      <td>2014-0250-c</td>\n",
       "      <td>nan</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000971</td>\n",
       "      <td>0</td>\n",
       "      <td>1.090328e-16</td>\n",
       "      <td>0</td>\n",
       "      <td>0.350669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1523</th>\n",
       "      <td>complainant stated he was assaulted by three u...</td>\n",
       "      <td>2014-0826-c</td>\n",
       "      <td>nan</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.004884</td>\n",
       "      <td>0</td>\n",
       "      <td>3.529409e-12</td>\n",
       "      <td>0</td>\n",
       "      <td>0.345093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>697</th>\n",
       "      <td>accused of refusing work assignment. employee ...</td>\n",
       "      <td>H-017-20</td>\n",
       "      <td>nan</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.453684</td>\n",
       "      <td>0</td>\n",
       "      <td>9.243473e-15</td>\n",
       "      <td>0</td>\n",
       "      <td>0.284905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1058</th>\n",
       "      <td>complainant's client was arrested for possessi...</td>\n",
       "      <td>2014-0067-n</td>\n",
       "      <td>nan</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.003686</td>\n",
       "      <td>0</td>\n",
       "      <td>3.360932e-11</td>\n",
       "      <td>0</td>\n",
       "      <td>0.351753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1099</th>\n",
       "      <td>complainant stated the officer constantly used...</td>\n",
       "      <td>2014-0140-c</td>\n",
       "      <td>nan</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.112272</td>\n",
       "      <td>0</td>\n",
       "      <td>7.921701e-10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.301040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1193</th>\n",
       "      <td>the complainant, who was the vietim of an atta...</td>\n",
       "      <td>2014-0273-c</td>\n",
       "      <td>nan</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.031558</td>\n",
       "      <td>0</td>\n",
       "      <td>6.532416e-05</td>\n",
       "      <td>0</td>\n",
       "      <td>0.344411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>678</th>\n",
       "      <td>failed to activate her body worn camera during...</td>\n",
       "      <td>c-042-21</td>\n",
       "      <td>nan</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0</td>\n",
       "      <td>3.208118e-27</td>\n",
       "      <td>0</td>\n",
       "      <td>0.341257</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>501 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        allegation_desc         meta label  \\\n",
       "1078  officer worked a paid detail in plain clothes ...  2014-0105-d   nan   \n",
       "1020  complaint was via webmail. police were called ...  2014-0001-n   nan   \n",
       "1374  officer accidentally left her service weapon u...  2014-0565-r   nan   \n",
       "1177  the complainant stated she was leaving her res...  2014-0250-c   nan   \n",
       "1523  complainant stated he was assaulted by three u...  2014-0826-c   nan   \n",
       "...                                                 ...          ...   ...   \n",
       "697   accused of refusing work assignment. employee ...     H-017-20   nan   \n",
       "1058  complainant's client was arrested for possessi...  2014-0067-n   nan   \n",
       "1099  complainant stated the officer constantly used...  2014-0140-c   nan   \n",
       "1193  the complainant, who was the vietim of an atta...  2014-0273-c   nan   \n",
       "678   failed to activate her body worn camera during...     c-042-21   nan   \n",
       "\n",
       "     target lr_scores  lr_scores_prob_1 sgd_scores  sgd_scores_prob_1  \\\n",
       "1078      1         0          0.098106          0       1.023363e-12   \n",
       "1020      1         0          0.001504          0       4.948917e-17   \n",
       "1374      1         0          0.000729          0       1.976547e-26   \n",
       "1177      1         0          0.000971          0       1.090328e-16   \n",
       "1523      1         0          0.004884          0       3.529409e-12   \n",
       "...     ...       ...               ...        ...                ...   \n",
       "697       1         0          0.453684          0       9.243473e-15   \n",
       "1058      1         0          0.003686          0       3.360932e-11   \n",
       "1099      1         0          0.112272          0       7.921701e-10   \n",
       "1193      1         0          0.031558          0       6.532416e-05   \n",
       "678       1         0          0.000059          0       3.208118e-27   \n",
       "\n",
       "     sgd_huber_scores  sgd_huber_scores_prob_1  \n",
       "1078                0                 0.264397  \n",
       "1020                0                 0.354349  \n",
       "1374                0                 0.273947  \n",
       "1177                0                 0.350669  \n",
       "1523                0                 0.345093  \n",
       "...               ...                      ...  \n",
       "697                 0                 0.284905  \n",
       "1058                0                 0.351753  \n",
       "1099                0                 0.301040  \n",
       "1193                0                 0.344411  \n",
       "678                 0                 0.341257  \n",
       "\n",
       "[501 rows x 10 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred = test_data[~((test_data.target == \"0\"))]\n",
    "test_pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.11 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f6c8f846148a3e4d140e6ddf63c190cff559dcf260a4a21539f0978f2b58638c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
